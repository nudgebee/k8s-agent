playbookRepos: {}

# sinks configurations
sinksConfig:
  - nudge_bee_sink:
      name: nudgebee_webhook_sink
      size_limit: 4380

# global parameters
clusterName: ""
clusterZone: ""

automountServiceAccountToken: true

globalConfig:
  grafana_url: ""
  grafana_api_key: ""
  grafana_dashboard_uid: ""
  prometheus_url: ""
  account_id: ""
  signing_key: ""
  custom_annotations: []

alertRelabel: []

# safe actions to enable authenticated users to run
lightActions:
- related_pods
- prometheus_enricher
- add_silence
- delete_pod
- delete_silence
- get_silences
- logs_enricher
- pod_events_enricher
- deployment_events_enricher
- job_events_enricher
- job_pod_enricher
- get_resource_yaml
- node_cpu_enricher
- node_disk_analyzer
- node_running_pods_enricher
- node_allocatable_resources_enricher
- node_status_enricher
- node_graph_enricher
- oomkilled_container_graph_enricher
- pod_oom_killer_enricher
- oomkilled_container_matrix_enricher
- pod_node_metrics_enricher
- noisy_neighbours_enricher
- oom_killer_enricher
- volume_analysis
- python_profiler
- pod_ps
- python_memory
- debugger_stack_trace
- python_process_inspector
- prometheus_alert
- create_pvc_snapshot
- resource_events_enricher
- delete_job
- list_resource_names
- node_dmesg_enricher
- status_enricher
- popeye_scan
- krr_scan
- kube_bench_scan
- handle_alertmanager_event
- drain
- cordon
- uncordon
- rollout_restart
- unused_pv
- image_scanner
- pod_enricher
- logs_enricher
- volume_analyzer

# install prometheus, opencost , along with nudgebee ?
enablePrometheusStack: false
enableOpenCostStack: false
enableServiceMonitors: false
monitorHelmReleases: false

# scale alerts processing.
# Used to support clusters with high load of alerts. When used, the runner will consume more memory
scaleAlertsProcessing: false

# Enable loading playbooks to a persistent volume
playbooksPersistentVolume: false
playbooksPersistentVolumeSize: 4Gi

# priority builtin playbooks for running before all playbooks
priorityBuiltinPlaybooks:
# playbooks for prometheus silencing
- triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - name_silencer:
      names: ["Watchdog", "KubeSchedulerDown", "KubeControllerManagerDown", "InfoInhibitor"]

# Silences for small/local clusters
- triggers:
  - on_prometheus_alert:
      status: "all"
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  actions:
  - name_silencer:
      names: ["etcdInsufficientMembers", "etcdMembersDown", "NodeClockNotSynchronising", "PrometheusTSDBCompactionsFailing"]

# Silences for specific providers
- triggers:
  - on_prometheus_alert:
      status: "all"
      k8s_providers: ["GKE"]
  actions:
  - name_silencer:
      names: ["KubeletDown"]

- triggers:
  - on_prometheus_alert:
      alert_name: CPUThrottlingHigh
      k8s_providers: ["DigitalOcean"]
      pod_name_prefix: "do-node-agent"
  actions:
  - silence_alert:
      log_silence: true

# Smart Silences
- triggers:
  - on_prometheus_alert:
      alert_name: TargetDown
  actions:
  - target_down_dns_silencer: {}

# custom user playbooks
customPlaybooks:
- triggers:
  - on_deployment_all_changes: {}
  - on_daemonset_all_changes: {}
  - on_statefulset_all_changes: {}
  - on_replicaset_all_changes: {}
  - on_pod_all_changes: {}
  - on_node_all_changes: {}
  - on_job_all_changes: {}
  actions:
  - resource_events_diff: {}
- triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 12 * * 1" # every Monday at 12:00
  actions:
  - popeye_scan:
      spinach: |
        popeye:
          excludes:
            v1/pods:
              - name: rx:kube-system
- triggers:
  - on_kubernetes_warning_event_create:
      exclude: ["NodeSysctlChange"]
  actions:
  - event_report: {}
  - event_resource_events: {}
- triggers:
  - on_job_failure: {}
  actions:
  - create_finding:
      aggregation_key: "job_failure"
      title: "Job Failed"
  - job_info_enricher: {}
  - job_events_enricher: {}
  - job_pod_enricher: {}
- triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 12 * * *"
  actions:
  - popeye_scan: {}
- triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 12 * * *"
  actions:
  - krr_scan: {}
- triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 0 * * *"
  actions:
  - unused_pv: {}
- triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 0 * * *" # every Monday at 10:00
  actions:
  - volume_analyzer: {}
- triggers:
  - on_schedule:
      fixed_delay_repeat:
        repeat: 10             # number of times to run or -1 to run forever
        seconds_delay: 3600    # seconds between each run
  actions:
  - krr_scan: {}
- triggers:
  - on_schedule:
      fixed_delay_repeat:
        repeat: 1              # number of times to run or -1 to run forever
        seconds_delay: 60      # seconds between each run
  actions:
  - popeye_scan: {}  
- triggers:
    - on_deployment_all_changes: {}
    - on_daemonset_all_changes: {}
    - on_statefulset_all_changes: {}
    - on_ingress_all_changes: {}
  actions:
    - resource_babysitter: {}

# builtin playbooks
builtinPlaybooks:
# playbooks for non-prometheus based monitoring
- triggers:
  - on_pod_crash_loop:
      restart_reason: "CrashLoopBackOff"
  actions:
  - report_crash_loop: {}
  - resource_events_enricher: {}
  - pod_enricher: {}

- triggers:
  - on_image_pull_backoff: {}
  actions:
  - image_pull_backoff_reporter: {}
  - resource_events_enricher: {}
  - pod_enricher: {}

# playbooks for non-prometheus based monitoring that use prometheus for enrichment
- triggers:
  - on_pod_oom_killed:
      rate_limit: 3600
  actions:
  - pod_oom_killer_enricher: {}
  - logs_enricher: {}
  - noisy_neighbours_enricher:
      resource_type: Memory
  - oomkilled_container_matrix_enricher:
      resource_type: Memory
      delay_graph_s: 60
  - pod_node_metrics_enricher:
      resource_type: Memory
  - pod_enricher: {}
  - resource_events_enricher: {}
  stop: true

# playbooks for prometheus alerts enrichment
- triggers:
  - on_prometheus_alert:
      alert_name: KubePodCrashLooping
  actions:
  - logs_enricher: {}
  - pod_events_enricher: {}
  - pod_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: PrometheusRuleFailures
  actions:
  - prometheus_rules_enricher: {}
  - logs_enricher:
      filter_regex: ".*Evaluating rule failed.*"

- triggers:
  - on_prometheus_alert:
      alert_name: KubeCPUOvercommit
  actions:
  - cpu_overcommited_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeMemoryOvercommit
  actions:
  - memory_overcommited_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubePodNotReady
  actions:
  - logs_enricher: {}
  - pod_events_enricher: {}
  - pod_issue_investigator: {}
  - pod_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeContainerWaiting
  actions:
  - pod_issue_investigator: {}
  - pod_events_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeHpaReplicasMismatch
  actions:
  - hpa_mismatch_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeJobFailed
  - on_prometheus_alert:
      alert_name: KubeJobCompletion
  - on_prometheus_alert:
      alert_name: KubeJobNotCompleted
  actions:
  - job_info_enricher: {}
  - job_events_enricher: {}
  - job_pod_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeAggregatedAPIDown
  actions:
  - api_service_status_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeletTooManyPods
  actions:
  - node_pods_capacity_enricher: {}
  - alert_explanation_enricher:
      alert_explanation: "The node is approaching the maximum number of scheduled pods."
      recommended_resolution: "Verify that you defined proper resource requests for your workloads. If pods cannot be scheduled, add more nodes to your cluster."

- triggers:
  - on_prometheus_alert:
      alert_name: KubeNodeNotReady
  actions:
  - node_allocatable_resources_enricher: {}
  - node_running_pods_enricher: {}
  - status_enricher:
      show_details: true
  - node_dmesg_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeNodeUnreachable
  actions:
  - resource_events_enricher: {}
  - node_status_enricher: {}

# Prometheus Statefulset playbooks
- triggers:
  - on_prometheus_alert:
      alert_name: KubeStatefulSetReplicasMismatch
  actions:
  - resource_events_enricher:
      dependent_pod_mode: true
  - statefulset_replicas_enricher: {}
  - pod_issue_investigator: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeStatefulSetUpdateNotRolledOut
  actions:
  - related_pods: {}
  - statefulset_replicas_enricher: {}


# Prometheus Daemonset playbooks
- triggers:
  - on_prometheus_alert:
      alert_name: KubeDaemonSetRolloutStuck
  actions:
  - resource_events_enricher: {}
  - related_pods: {}
  - daemonset_status_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDaemonsetMisscheduled
  - on_prometheus_alert:
      alert_name: KubeDaemonSetMisScheduled
  actions:
  - daemonset_status_enricher: {}
  - daemonset_misscheduled_analysis_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: HostHighCpuLoad
  actions:
  - node_cpu_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: HostOomKillDetected
  actions:
  - oom_killer_enricher: {}
  - alert_graph_enricher:
      resource_type: Memory
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: NodeFilesystemSpaceFillingUp
  - on_prometheus_alert:
      alert_name: NodeFilesystemAlmostOutOfSpace
  actions:
  - node_disk_analyzer: {}
  - alert_graph_enricher:
      resource_type: Disk
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: CPUThrottlingHigh
      status: "all" # sometimes this enricher silences the alert, so we need to silence it regardless of status
  actions:
  - cpu_throttling_analysis_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Pod
- triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDeploymentReplicasMismatch
  - on_prometheus_alert:
      alert_name: KubeDeploymentReplicasMismatch
  actions:
  - pod_issue_investigator: {}
  - deployment_events_enricher:
      included_types: ["Warning"]
  - deployment_events_enricher:
      included_types: ["Warning", "Normal"]
      dependent_pod_mode: true

- triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - default_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: NodeFilesystemSpaceFillingUp
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  - on_prometheus_alert:
      alert_name: NodeFilesystemAlmostOutOfSpace
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  actions:
  - alert_explanation_enricher:
      alert_explanation: "This alert is fired when the file system is running out of space."
      recommended_resolution: "This is a common issue on local clusters and we recommend increasing the node disk size for your cluster to run optimally."
      
# parameters for the nudgebee forwarder deployment
kubewatch:
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/kubewatch:v2.5
  imagePullPolicy: IfNotPresent
  pprof: true
  resources:
    requests:
      cpu: 10m
      memory: 512Mi
    limits:
      cpu: ~
  additional_env_vars: []
  tolerations: []
  annotations: {}
  nodeSelector: ~
  imagePullSecrets: []
  config:
    namespace: ""
    resource:
      deployment: true
      replicationcontroller: false  # 0.10.12 disabled because not supported on the runner
      replicaset: true
      daemonset: true
      statefulset: true
      services: true
      pod: true
      job: true
      node: true
      hpa: true
      clusterrole: true
      clusterrolebinding: true
      serviceaccount: true
      persistentvolume: true
      namespace: true
      configmap: true # 0.9.17
      secret: false       # disabled for security reasons
      event: true  # updated on kubewatch 2.5
      coreevent: false # added on kubewatch 2.5
      ingress: true # full support on kubewatch 2.4 (earlier versions have ingress bugs)

# parameters for the renderer service used in nudgebee runner to render grafana graphs
grafanaRenderer:
  enableContainer: false
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/grafana-renderer:7
  imagePullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: ~

# parameters for the nudgebee runner service account
runnerServiceAccount:
  # image pull secrets added to the runner service account. Any pod using the service account will get those
  imagePullSecrets: []

# parameters for the nudgebee runner
runner:
  image: public.ecr.aws/p0t0i2l3/nudgebee-agent:latest
  imagePullPolicy: Always
  log_level: INFO
  resources:
    requests:
      cpu: 250m
      memory: 1024Mi
    limits:
      cpu: ~
  additional_env_froms: []
  tolerations: []
  annotations: {}
  nodeSelector: ~
  customClusterRoleRules: []
  imagePullSecrets: []
  extraVolumes: []
  extraVolumeMounts: []
  krr_image_override: public.ecr.aws/p0t0i2l3/krr-public:v1.0.0
  relay_address: wss://relay.dev.nudgebee.pollux.in/register
  nudgebee:
    endpoint: https://collector.dev.nudgebee.pollux.in
    publish_window: '900'
    auth_secret_key: ""

kube-prometheus-stack:
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ '...' ]
        group_wait: 1s
        group_interval: 1s
        repeat_interval: 4h
        receiver: 'nudgebee'
        routes:
          - match:
              alertname: Watchdog
            receiver: 'null'
      receivers:
        - name: 'null'
        - name:  'nudgebee'
          webhook_configs:
            - url: 'http://{{ .Release.Name }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/api/alerts'
              send_resolved: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
  kubeProxy:
    enabled: false
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
          memory: 2Gi
        limits:
          memory: 2Gi
      retention: 15d
      # we set a value slightly lower than the 100Gi below
      # the retentionSize uses the suffix GB but it is really Gi units
      # that is, the retentionSize is measured in base2 units just like Gi, Mi, etc
      retentionSize: "1GB"
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
  prometheus-node-exporter:
    service:
      port: 9104
      targetPort: 9104
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        memory: 50Mi
    # disable node-exporter on fargate because fargate doesn't allow daemonsets
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                    - fargate
  prometheusOperator:
    resources:
      requests:
        cpu: 100m
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 0
  kube-state-metrics:
    resources:
      requests:
        cpu: 10m
        memory: 256Mi
      limits:
        memory: 256Mi

opencost:
  opencost: 
    prometheus: 
      internal:
        namespaceName: '{{ .Release.Namespace }}'
        serviceName: '{{ .Release.Name }}-kube-p-prometheus'
        port: 9090
    ui.enabled: false

rsa: ~
# custom parameters for OpenShift clusters
openshift:
  enabled: false
  createScc: false
  createPrivilegedScc: false
  privilegedSccName: null
  sccName: null
  sccPriority: null
  privilegedSccPriority: null