# Global name overrides for all resources
nameOverride: ""
fullnameOverride: ""
playbookRepos: {}
# sinks configurations
sinksConfig:
  - nudge_bee_sink:
      name: nudgebee_webhook_sink
      size_limit: 4380
# global parameters
clusterName: ""
clusterZone: ""
automountServiceAccountToken: true
globalConfig:
  grafana_url: ""
  grafana_api_key: ""
  grafana_dashboard_uid: ""
  prometheus_url: ""
  account_id: ""
  signing_key: ""
  custom_annotations: {}
  regex_replacement_style: SAME_LENGTH_ASTERISKS
  regex_replacer_patterns:
    - name: JWT
      regex: "^(?:[\\w-]*\\.){2}[\\w-]*$"
    - name: BASE64
      regex: "^([A-Za-z0-9+/]{4})*([A-Za-z0-9+/]{3}=|[A-Za-z0-9+/]{2}==)?$"
    - name: google-api-key
      regex: "AIza[0-9A-Za-z-_]{35}"
    - name: google-auth-code
      regex: "1/[0-9A-Za-z-]{43}|1/[0-9A-Za-z-]{64}"
alertRelabel: []
# safe actions to enable authenticated users to run
lightActions:
  - related_pods
  - prometheus_enricher
  - add_silence
  - delete_pod
  - delete_silence
  - get_silences
  - logs_enricher
  - pod_events_enricher
  - deployment_events_enricher
  - job_events_enricher
  - job_pod_enricher
  - get_resource_yaml
  - node_cpu_enricher
  - node_disk_analyzer
  - node_running_pods_enricher
  - node_allocatable_resources_enricher
  - node_status_enricher
  - node_graph_enricher
  - oomkilled_container_graph_enricher
  - pod_oom_killer_enricher
  - oomkilled_container_matrix_enricher
  - pod_node_metrics_enricher
  - noisy_neighbours_enricher
  - oom_killer_enricher
  - volume_analysis
  - python_profiler
  - pod_ps
  - python_memory
  - debugger_stack_trace
  - python_process_inspector
  - prometheus_alert
  - create_pvc_snapshot
  - resource_events_enricher
  - delete_job
  - list_resource_names
  - node_dmesg_enricher
  - status_enricher
  - popeye_scan
  - krr_scan
  - kube_bench_scan
  - handle_alertmanager_event
  - drain
  - cordon
  - uncordon
  - rollout_restart
  - unused_pv
  - image_scanner
  - pod_enricher
  - logs_enricher
  - volume_analyzer
  - replica_rightsizing
  - foreign_logs_enricher
  - replace_workload
  - delete_workload
  - pod_bash_enricher
  - node_bash_enricher
  - pod_script_run_enricher
  - python_debugger
  - prometheus_labels
  - create_or_replace_alert_rule
  - pod_node_event_enricher
  - pod_profiler
  - delete_alert_rule
  - pod_ephemeral_enricher
  - get_resource
  - prometheus_queries_enricher
  - k8s_version_upgrade
  - helm_chart_upgrade
  - certificate_scanner
  - trivy_cis_scan
  - create_workload
  - rightsize_pvc
  - continuous_rightsizing
  - kubectl_command_executor
  - list_k8s_pdb
  - k8s_helm_compatibility_check
  - pod_node_metrics_enricher
  - api_traces_enricher
  - neighboring_connected_logs_enricher
  - api_traces_enricher_v2
  - pod_metric_enricher
  - neighboring_workload_health_enricher
# enable prometheus auto-discovery (set to false to disable auto-discovery and require prometheus_url configuration)
enablePrometheusStack: true
# install opencost , along with nudgebee ?
enableOpenCostStack: false
enableServiceMonitors: true
monitorHelmReleases: false
# scale alerts processing.
# Used to support clusters with high load of alerts. When used, the runner will consume more memory
scaleAlertsProcessing: false
# Enable loading playbooks to a persistent volume
playbooksPersistentVolume: false
playbooksPersistentVolumeSize: 4Gi
# priority builtin playbooks for running before all playbooks
priorityBuiltinPlaybooks:
  # playbooks for prometheus silencing
  - triggers:
      - on_prometheus_alert:
          status: "all"
    actions:
      - name_silencer:
          names: ["Watchdog", "KubeSchedulerDown", "KubeControllerManagerDown", "InfoInhibitor"]
  # Silences for small/local clusters
  - triggers:
      - on_prometheus_alert:
          status: "all"
          k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
    actions:
      - name_silencer:
          names: ["etcdInsufficientMembers", "etcdMembersDown", "NodeClockNotSynchronising", "PrometheusTSDBCompactionsFailing"]
  # Silences for specific providers
  - triggers:
      - on_prometheus_alert:
          status: "all"
          k8s_providers: ["GKE"]
    actions:
      - name_silencer:
          names: ["KubeletDown"]
  - triggers:
      - on_prometheus_alert:
          alert_name: CPUThrottlingHigh
          k8s_providers: ["DigitalOcean"]
          pod_name_prefix: "do-node-agent"
    actions:
      - silence_alert:
          log_silence: true
  # Smart Silences
  - triggers:
      - on_prometheus_alert:
          alert_name: TargetDown
    actions:
      - target_down_dns_silencer: {}
# custom user playbooks
customPlaybooks:
  - triggers:
      - on_deployment_all_changes: {}
      - on_daemonset_all_changes: {}
      - on_statefulset_all_changes: {}
      - on_replicaset_all_changes: {}
      - on_pod_all_changes: {}
      - on_node_all_changes: {}
      - on_job_all_changes: {}
      - on_rollout_all_changes: {}
    actions:
      - resource_events_diff: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 12 * * 1" # every week on monday at 02:00
    actions:
      - popeye_scan:
          spinach: |
            popeye:
              excludes:
                v1/pods:
                  - name: rx:kube-system
  - triggers:
      - on_kubernetes_warning_event_create:
          exclude: ["NodeSysctlChange"]
    actions:
      - event_report: {}
      - event_resource_events: {}
  - triggers:
      - on_job_failure: {}
    actions:
      - create_finding:
          aggregation_key: "job_failure"
          title: "Job Failed"
      - job_info_enricher: {}
      - job_events_enricher: {}
      - job_pod_enricher: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 12 * * *" # every day at 12:00
    actions:
      - krr_scan: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 12 * * *" # every day at 12:00
    actions:
      - certificate_scanner: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 0 * * 1" # At 00:00 on every monday
    actions:
      - k8s_version_upgrade: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 0 * * 1" # At 00:00 on every monday
    actions:
      - k8s_helm_compatibility_check: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 12 * * *" # every day at 12:00
    actions:
      - helm_chart_upgrade: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 0 * * *" # every day at 00:00
    actions:
      - unused_pv: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 */4 * * *" # every 4 hour
    actions:
      - volume_analyzer: {}
  - triggers:
      - on_deployment_all_changes: {}
      - on_daemonset_all_changes: {}
      - on_statefulset_all_changes: {}
      - on_ingress_all_changes: {}
      - on_rollout_all_changes: {}
    actions:
      - resource_babysitter: {}
  - triggers:
      - on_schedule:
          cron_schedule_repeat:
            cron_expression: "0 8 * * 1" # every monday at 8:00
    actions:
      - trivy_cis_scan: {}
# builtin playbooks
builtinPlaybooks:
  # playbooks for non-prometheus based monitoring
  - triggers:
      - on_pod_crash_loop:
          restart_reason: "CrashLoopBackOff"
    actions:
      - report_crash_loop: {}
      - resource_events_enricher: {}
      - pod_enricher: {}
      - logs_enricher:
          previous: true
      - impacted_services_enricher: {}
  - triggers:
      - on_image_pull_backoff: {}
    actions:
      - image_pull_backoff_reporter: {}
      - resource_events_enricher: {}
      - pod_enricher: {}
  # playbooks for non-prometheus based monitoring that use prometheus for enrichment
  - triggers:
      - on_pod_oom_killed:
          rate_limit: 3600
    actions:
      - pod_oom_killer_enricher: {}
      - logs_enricher:
          previous: true
      - noisy_neighbours_enricher:
          resource_type: Memory
      - oomkilled_container_matrix_enricher:
          resource_type: Memory
          delay_graph_s: 30
      - pod_node_metrics_enricher:
          resource_type: Memory
      - pod_node_metrics_enricher:
          resource_type: MemoryRequest
      - pod_enricher: {}
      - resource_events_enricher: {}
      - pod_node_event_enricher: {}
    stop: true
  # playbooks for prometheus alerts enrichment
  - triggers:
      - on_prometheus_alert:
          alert_name: KubePodCrashLooping
    actions:
      - logs_enricher:
          previous: true
      - pod_events_enricher: {}
      - pod_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: PrometheusRuleFailures
    actions:
      - prometheus_rules_enricher: {}
      - logs_enricher:
          filter_regex: ".*Evaluating rule failed.*"
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeCPUOvercommit
    actions:
      - cpu_overcommited_enricher: {}
      - cluster_cpu_requests_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeMemoryOvercommit
    actions:
      - memory_overcommited_enricher: {}
      - cluster_memory_requests_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubePodNotReady
    actions:
      - logs_enricher: {}
      - pod_events_enricher: {}
      - pod_issue_investigator: {}
      - pod_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeContainerWaiting
    actions:
      - pod_issue_investigator: {}
      - pod_events_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeHpaReplicasMismatch
    actions:
      - hpa_mismatch_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeJobFailed
      - on_prometheus_alert:
          alert_name: KubeJobCompletion
      - on_prometheus_alert:
          alert_name: KubeJobNotCompleted
    actions:
      - job_info_enricher: {}
      - job_events_enricher: {}
      - job_pod_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeAggregatedAPIDown
    actions:
      - api_service_status_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeletTooManyPods
    actions:
      - node_pods_capacity_enricher: {}
      - alert_explanation_enricher:
          alert_explanation: "The node is approaching the maximum number of scheduled pods."
          recommended_resolution: "Verify that you defined proper resource requests for your workloads. If pods cannot be scheduled, add more nodes to your cluster."
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeNodeNotReady
    actions:
      - node_allocatable_resources_enricher: {}
      - node_running_pods_enricher: {}
      - status_enricher:
          show_details: true
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeNodeUnreachable
    actions:
      - resource_events_enricher: {}
      - node_status_enricher: {}
  # Prometheus Statefulset playbooks
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeStatefulSetReplicasMismatch
    actions:
      - resource_events_enricher:
          dependent_pod_mode: true
      - statefulset_replicas_enricher: {}
      - pod_issue_investigator: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeStatefulSetUpdateNotRolledOut
    actions:
      - related_pods: {}
      - statefulset_replicas_enricher: {}
  # Prometheus Daemonset playbooks
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeDaemonSetRolloutStuck
    actions:
      - resource_events_enricher:
          dependent_pod_mode: true
      - related_pods: {}
      - daemonset_status_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubernetesDaemonsetMisscheduled
      - on_prometheus_alert:
          alert_name: KubeDaemonSetMisScheduled
    actions:
      - daemonset_status_enricher: {}
      - daemonset_misscheduled_analysis_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: HostHighCpuLoad
    actions:
      - node_cpu_enricher: {}
      - alert_graph_enricher:
          resource_type: CPU
          item_type: Node
  - triggers:
      - on_prometheus_alert:
          alert_name: HostOomKillDetected
    actions:
      - oom_killer_enricher: {}
      - alert_graph_enricher:
          resource_type: Memory
          item_type: Node
  - triggers:
      - on_prometheus_alert:
          alert_name: KubePersistentVolumeFillingUp
    actions:
      - prometheus_pvc_event_enricher: {}
    sinks:
      - "nudgebee_webhook_sink"
  - triggers:
      - on_prometheus_alert:
          alert_name: KubernetesVolumeOutOfDiskSpace
    actions:
      - prometheus_pvc_event_enricher: {}
    sinks:
      - "nudgebee_webhook_sink"
  - triggers:
      - on_prometheus_alert:
          alert_name: CPUThrottlingHigh
          status: "all" # sometimes this enricher silences the alert, so we need to silence it regardless of status
    actions:
      - cpu_throttling_analysis_enricher: {}
      - pod_metric_enricher:
          resource_type: CPU
      - pod_node_metrics_enricher:
          resource_type: CPU
      - pod_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubernetesDeploymentReplicasMismatch
      - on_prometheus_alert:
          alert_name: KubeDeploymentReplicasMismatch
    actions:
      - pod_issue_investigator: {}
      - deployment_events_enricher: {}
      - deployment_events_enricher:
          dependent_pod_mode: true
  - triggers:
      - on_prometheus_alert:
          status: "all"
    actions:
      - default_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeDeploymentRolloutStuck
    actions:
      - deployment_events_enricher: {}
      - deployment_events_enricher:
          dependent_pod_mode: true
      - pod_issue_investigator: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: NodeFilesystemSpaceFillingUp
          k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
      - on_prometheus_alert:
          alert_name: NodeFilesystemAlmostOutOfSpace
          k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
    actions:
      - alert_explanation_enricher:
          alert_explanation: "This alert is fired when the file system is running out of space."
          recommended_resolution: "This is a common issue on local clusters and we recommend increasing the node disk size for your cluster to run optimally."
  - triggers:
      - on_prometheus_alert:
          alert_name: HighErrorCriticalLogs
    actions:
      - logs_enricher: {}
      - pod_enricher: {}
      - impacted_services_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: ApplicationAPIFailures
    actions:
      - logs_enricher: {}
      - pod_enricher: {}
      - api_failure_enricher: {}
      - api_traces_enricher: {}
      - neighboring_connected_logs_enricher:
          tail_lines: 100
  - triggers:
      - on_prometheus_alert:
          alert_name: PodMemoryReachingLimit
    actions:
      - logs_enricher: {}
      - pod_enricher: {}
      - pod_node_metrics_enricher:
          resource_type: Memory
      - pod_metric_enricher:
          resource_type: Memory
  - triggers:
      - on_prometheus_alert:
          alert_name: KubeVersionMismatch
    actions:
      - node_semantic_version_mismatch_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubePersistentVolumeFillingUp
    actions:
      - prometheus_pvc_event_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: KubernetesVolumeOutOfDiskSpace
    actions:
      - prometheus_pvc_event_enricher: {}
  - triggers:
      - on_prometheus_alert:
          alert_name: NodeMemoryHighUtilization
    actions:
      - node_allocatable_resources_enricher: {}
      - node_running_pods_enricher: {}
      - status_enricher:
          show_details: true
  - triggers:
      - on_prometheus_alert:
          alert_name: NodeCPUHighUsage
    actions:
      - node_allocatable_resources_enricher: {}
      - node_running_pods_enricher: {}
      - status_enricher:
          show_details: true
  - triggers:
      - on_prometheus_alert:
          alert_name: TCPConnectionFailure
    actions:
      - logs_enricher: {}
      - pod_enricher: {}
      - tcp_connection_failure_enricher: {}
# parameters for the nudgebee forwarder deployment
kubewatch:
  image:
    repository: registry.nudgebee.com/kubewatch
    tag: 2025-08-05T15-21-58_244b8879e108cf4b12313d400cd3716265869344
  imagePullPolicy: IfNotPresent
  pprof: true
  resources:
    requests:
      cpu: 10m
      memory: 512Mi
    limits:
      cpu: ~
  additional_env_vars: []
  tolerations: []
  annotations: {}
  nodeSelector: ~
  imagePullSecrets: []
  config:
    namespace: ""
    resource:
      deployment: true
      replicationcontroller: false # 0.10.12 disabled because not supported on the runner
      replicaset: true
      daemonset: true
      statefulset: true
      services: true
      pod: true
      job: true
      node: true
      hpa: true
      clusterrole: true
      clusterrolebinding: true
      serviceaccount: true
      persistentvolume: true
      namespace: true
      configmap: true # 0.9.17
      secret: false # disabled for security reasons
      event: true # updated on kubewatch 2.5
      coreevent: false # added on kubewatch 2.5
      ingress: true # full support on kubewatch 2.4 (earlier versions have ingress bugs)
      rollout: true
    sync:
      # Enable/disable scheduled sync
      enabled: true
      # How often to run the sync
      interval: "1h"
      # Enable/disable payload compression for sync data
      compression: true
      # Number of resources to send in a single batch
      batchSize: 1000
      # Timeout in seconds for the sync request
      timeoutSeconds: 180
      # Configuration for workload snapshot
      workloadSnapshot:
        # Enable/disable workload snapshot as part of the sync
        enabled: true
        # Include nodes in the workload snapshot
        includeNodes: true
        # Include services in the workload snapshot
        includeServices: true
# parameters for the renderer service used in nudgebee runner to render grafana graphs
grafanaRenderer:
  enableContainer: false
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/grafana-renderer:7
  imagePullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: ~
# parameters for the nudgebee runner service account
runnerServiceAccount:
  # image pull secrets added to the runner service account. Any pod using the service account will get those
  imagePullSecrets: []
# parameters for the nudgebee runner
runner:
  image:
    repository: registry.nudgebee.com/nudgebee-agent
    tag: 2026-01-09T10-54-01_eabafd546d9e50c1601a8262667a223226abe042
  imagePullPolicy: IfNotPresent
  log_level: INFO
  resources:
    requests:
      cpu: 250m
      memory: 1000Mi
    limits:
      cpu: ~
  tolerations: []
  annotations: {}
  nodeSelector: ~
  customClusterRoleRules: []
  imagePullSecrets: []
  extraVolumes: []
  extraVolumeMounts: []
  image_registry: registry.nudgebee.com
  krr_image_override: krr-public:2025-07-10T07-36-19_9195d324fdcde4e2f4724ba270c4351ed2366a1d
  relay_address: wss://relay.nudgebee.com/register
  profiler_image_override: 2025-10-07T09-20-18_6ede7487ac41f9c167a2e547e70ac7e7a2de7a88
  kubepug_image_override: kubepug:2025-08-13T08-26-24_d16f6f2d1e27c24258c36ab8caf2054b583aa3cb
  nova_image_override: nova:2025-08-25T06-49-02_8a8e0766cfc5817c4de429f0df8fb0faa154907f
  clickhouse_enabled: true
  clickhouse_secret: ""
  runbook_sidecar_image_tag: 2026-01-09T08-56-51_1bc8ed44579b442306416c9a2733c4ae4c124873
  victoria_metrics_enabled: false
  loki:
    url: ""
    username: ""
    password: ""
    headers: ""
  es:
    enabled: false
    url: "https://elasticsearch-es-internal-http.monitoring.svc:9200"
    apiKey: ""
    headers: ""
  signoz:
    enabled: false
    url: "https://signoz.signoz.svc:8080"
    apiKey: ""
    user_email: ""
    user_password: ""
  grafana:
    enabled: false
    url: ""
    apiKey: ""
    username: ""
    password: ""
  additional_env_vars:
    - name: POPEYE_IMAGE_OVERRIDE
      value: "popeye:v0.11.1"
    - name: CLICKHOUSE_PORT
      value: "8123"
    - name: CLICKHOUSE_USER
      value: default
    - name: CLICKHOUSE_DB
      value: default
  nudgebee:
    endpoint: https://collector.nudgebee.com
    publish_window: "3600"
    auth_secret_key: ""
  serviceAccount:
    annotations: {}
apiServer:
  enabled: true
  image:
    repository: ~ # inherits from runner.image.repository
    tag: ~ # inherits from runner.image.tag
  imagePullPolicy: ~ # inherits from runner.imagePullPolicy
  log_level: ~ # inherits from runner.log_level
  resources:
    requests:
      cpu: 250m
      memory: 1000Mi
    limits:
      cpu: ~
      memory: ~
  tolerations: []
  annotations: {}
  nodeSelector: ~
  affinity: ~
  imagePullSecrets: ~ # inherits from runner.imagePullSecrets
  extraVolumes: []
  extraVolumeMounts: []
  additional_env_vars: []
  additional_env_froms: []
  sendAdditionalTelemetry: ~
  krr_image_override: ~
  image_registry: ~
  relay_address: ~
  profiler_image_override: ~
  kubepug_image_override: ~
  nova_image_override: ~
  runbook_sidecar_image_tag: ~
  victoria_metrics_enabled: ~
  clickhouse_enabled: ~
  clickhouse_secret: ~
nodeAgent:
  enabled: true
  podmonitor:
    enabled: true
    azuremanaged: false
  podAnnotations: {}
  image:
    repository: registry.nudgebee.com/nudgebee-node-agent
    pullPolicy: IfNotPresent
    tag: 2026-01-09T09-03-36_0a09bddf3139c604c8c714f14d6c08a222d76ae4
  resources:
    requests:
      cpu: "100m"
      memory: "500Mi"
    limits:
      cpu: "1"
      memory: "1Gi"
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  priorityClassName: ""
  logsEndpoint: ""
  profilesEndpoint: ""
  tracesEndpoint: "" # Will be set dynamically based on name overrides
  env:
    - name: SENSITIVE_HEADERS
      value: "Authorization,Proxy-Authorization,Cookie,Set-Cookie,X-Auth-Token,X-CSRF-Token,X-Session-ID,X-JWT-Token,X-Api-Key,X-Api-Token,X-Access-Token,X-Secret-Token,X-Refresh-Token,X-User-Token,X-Firebase-Auth,X-Google-Auth,X-Authorization,X-Wsse,X-WebService-Token,Authentication,Proxy-Authentication,X-Authentication-Token,X-Device-ID,X-Device-Token,X-Device-Key"
opencost:
  # Note: opencost requires Prometheus. If enablePrometheusStack is false, this should also be false
  enabled: true
  nameOverride: ""
  fullnameOverride: ""
  opencost:
    exporter:
      image:
        registry: registry.nudgebee.com
        repository: opencost
        tag: 1.114.0
    prometheus:
      external:
        url: ""
        enabled: true
      internal:
        enabled: false
      serviceMonitor:
        enabled: true
      defaultRules:
        create: true
    service:
      labels:
        app: opencost
    ui:
      enabled: false
rsa: ~
# custom parameters for OpenShift clusters
openshift:
  enabled: false
  createScc: true
  createPrivilegedScc: false
  privilegedSccName: null
  sccName: null
  sccPriority: null
  privilegedSccPriority: null
opentelemetry-collector:
  enabled: true
  image:
    repository: registry.nudgebee.com/opentelemetry-collector-contrib
    tag: 0.75.0
  # Inherit name overrides from parent chart
  nameOverride: ""
  fullnameOverride: ""
  resources:
    limits:
      memory: 1024Mi
    requests:
      cpu: 250m
      memory: 512Mi
  mode: deployment
  extraEnvs:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          name: "nudgebee-agent-clickhouse" # Default name, can be overridden
          key: admin-password
  ports:
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
  config:
    processors:
      probabilistic_sampler:
        sampling_percentage: 10
      filter/drop_namespaces:
        error_mode: ignore
        traces:
          span:
            - attributes["k8s.namespace.name"] == "kube-system"
      filter/drop_health_check:
        error_mode: ignore
        traces:
          span:
            - attributes["http.route"] == "/health"
            - attributes["http.route"] == "/healthz"
            - attributes["http.route"] == "/live"
            - attributes["http.route"] == "/ready"
            - attributes["http.route"] == "/metrics"
            - attributes["http.target"] == "/health"
            - attributes["http.target"] == "/healthz"
            - attributes["http.target"] == "/live"
            - attributes["http.target"] == "/ready"
            - attributes["http.target"] == "/metrics"
            - attributes["url.path"] == "/health"
            - attributes["url.path"] == "/healthz"
            - attributes["url.path"] == "/live"
            - attributes["url.path"] == "/ready"
            - attributes["url.path"] == "/metrics"
      batch:
        timeout: 5s
        send_batch_size: 25000
    exporters:
      clickhouse:
        endpoint: "tcp://nudgebee-agent-clickhouse:9000?dial_timeout=10s&compress=lz4" # Default endpoint, can be overridden
        database: default
        ttl_days: 7
        username: default
        password: ${env:CLICKHOUSE_PASSWORD}
        logs_table_name: otel_logs
        traces_table_name: otel_traces
        metrics_table_name: otel_metrics
        timeout: 5s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
    service:
      pipelines:
        traces:
          processors: [filter/drop_namespaces, filter/drop_health_check, probabilistic_sampler, batch]
          exporters: [clickhouse]
          receivers: [otlp]
        logs:
          processors: [batch]
          exporters: [clickhouse]
          receivers: [otlp]
        metrics:
          processors: [batch]
          exporters: [clickhouse]
          receivers: [otlp]
clickhouse:
  enabled: true
  image:
    registry: registry.nudgebee.com
    repository: clickhouse
    tag: 23.3.1-debian-11-r0
  nameOverride: ""
  fullnameOverride: ""
  persistence:
    size: 50Gi
  replicaCount: 1
  shards: 1
  zookeeper:
    enabled: false
  auth:
    username: default
    password: "Nudgebee@123"
  extraOverrides: "<clickhouse>\n  <asynchronous_metric_log remove=\"1\"/>\n  <metric_log remove=\"1\"/>\n  <query_log remove=\"1\" />\n  <query_thread_log remove=\"1\" />  \n  <query_views_log remove=\"1\" />\n  <part_log remove=\"1\"/>\n  <text_log remove=\"1\" />\n  <trace_log remove=\"1\"/>\n  <opentelemetry_span_log remove=\"1\"/>\n</clickhouse>\n"
  resources:
    requests:
      cpu: 100m
      memory: 2000Mi
    limits:
      memory: 2000Mi
  initContainers:
    - name: volume-init
      image: registry.nudgebee.com/bitnami-os-shell:12-debian-12-r46
      command:
        - /bin/sh
        - -ec
        - |
          mkdir -p /bitnami/clickhouse/data
          chmod 700 /bitnami/clickhouse/data
          chown 1001:1001 /bitnami/clickhouse
          find /bitnami/clickhouse -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | \
          xargs -r chown -R 1001:1001
          echo "ClickHouse data directory initialized"
      securityContext:
        runAsUser: 0
      volumeMounts:
        - name: data
          mountPath: /bitnami/clickhouse
        - name: empty-dir
          mountPath: /tmp
          subPath: tmp-dir
  extraVolumes:
    - name: empty-dir
      emptyDir: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s
    prometheusRule:
      enabled: true
      rules:
        - alert: ClickHouseInstanceDown
          expr: up{job="clickhouse"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "ClickHouse instance is down"
            description: "ClickHouse node is not responding for more than 2 minutes."
        - alert: ClickHouseHighLatency
          expr: rate(clickhouse_query_duration_seconds_sum[5m]) / rate(clickhouse_query_duration_seconds_count[5m]) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High query latency detected on ClickHouse"
            description: "Average query duration is above 1 second over last 5 minutes."
alertmanager:
  create_nb_default_rules: true
  create_nb_alert_config: true
